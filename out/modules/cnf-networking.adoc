// Metadata created by nebel
//
// ConvertedFromFile: cnf-reqs_1.3_single.adoc
// ConversionStatus: raw

[id="cnf-networking"]
= Networking Overview

OpenShift is a multi-tenant environment. NFs will be deployed within a single namespace. Supporting applications like an OAM platform for multiple NFs from the same vendor should be run in an additional separate namespace.

Multus may be supported within the platform for additional NICs within containers. However Multus should be used only for those cases that cannot be supported, for example, by a F5 load balancer.

The POD and Services networks may have an unrouted address space, they are only reachable via service VIPs on the load balancers. The POD network may be NATed as traffic egresses the load balancer. Traffic inbound will be destination NATed to Service/Pod IP addresses.

Applications should use Network Policies for firewalling the application. Network Policies should be written with a default deny and only allow ports and protocols on an as needed basis for any pods and services.

.OVN-kubernetes CNI

OVN is Red Hat's CNI for pod networking. It is a Geneve based overlay that requires L3 reachability between the host nodes. This L3 reachability can be over L2 or a pre-existing overlay network. Openshift's OVN forwarding is based on flow rules and implemented with nftables on the host OS CNI POD.

.User Plane Functions

*Performance Addon Operator (PAO)*

Red Hat created the link:https://docs.openshift.com/container-platform/4.7/scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.html[Performance Addon Operator] for low latency nodes. The emergence of Edge computing in the area of Telco / 5G plays a key role in reducing latency and congestion problems and improving application performance. Many of the deployed applications in the Telco space require low latency that can only tolerate zero packet loss. OpenShift Container Platform provides a Performance Addon Operator to implement automatic tuning to achieve low latency performance for applications. The PAO is a meta operator that leverages MachineConfig, Topology Manager, CPU Manager, Tuned and KubeletConfig to optimize the nodes. The PAO enables:

* Hugepages
* Dynamic CPU isolation
* NUMA Awareness

*Hugepages*

In the Openshift Container Platform, nodes/hosts must pre-allocate huge pages. All workers within a cluster may have 32,000, 2M hugepages per NUMA node enabled:

----
 hugepages:
    defaultHugepagesSize: "2M"
    pages:
    - size: "2M"
      count: 32000
      node: 0
    - size: "2M"
      count: 32000
      node: 1
----

To request hugepages, pods must supply the following within the pod.spec for each container:

----
   resources:
      limits:
        hugepages-2Mi: 100Mi
        memory: "1Gi"
        cpu: "1"
      requests:
        hugepages-2Mi: 100Mi
        memory: "1Gi"
        cpu: "1"
----

For further reading on OCP's support of huge pages, see the Configuring huge pages documentation below..

https://docs.openshift.com/container-platform/4.7/scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.html#cnf-configuring-huge-pages_cnf-master[[.underline]#https://docs.openshift.com/container-platform/4.7/scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.html#cnf-configuring-huge-pages_cnf-master#]

*CPU Isolation*

The Performance Addon Operator manages host CPUs by dividing them into reserved CPUs for cluster and operating system housekeeping duties, and isolated CPUs for workloads. CPUs that are used for low latency workloads are set as isolated.

Device interrupts are load balanced between all isolated and reserved CPUs to avoid CPUs being overloaded, with the exception of CPUs where there is a guaranteed pod running. Guaranteed pod CPUs are prevented from processing device interrupts when the relevant annotations are set for the pod.

Worker nodes may have the following CPU profile applied, reserving 2 Cores per socket for housekeeping (kernel) and the rest for workloads.

----
spec:
  cpu:
    isolated: 4-39,44-79
    reserved: 0-3,40-43
----


* isolated - Has the lowest latency. Processes in this group have no interruptions and so can, for example, reach much higher DPDK zero packet loss bandwidth.
* reserved - The housekeeping CPUs. Threads in the reserved group tend to be very busy, so latency-sensitive applications should be run in the isolated group

Default worker node performanceprofile that may be enabled

----
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: perf-profile-2m-worker
spec:
  cpu:
    isolated: 4-39,44-79
    reserved: 0-3,40-43
  hugepages:
    defaultHugepagesSize: "2M"
    pages:
    - size: "2M"
      count: 32000
      node: 0
    - size: "2M"
      count: 32000
      node: 1
  numa:
    topologyPolicy: best-effort
  realTimeKernel:
    enabled: false
  nodeSelector:
    node-role.kubernetes.io/workerperf: ""
----

The resulting KubeletConfig: (partial config shown below)

----
{
  "kind": "KubeletConfiguration",
...
  "cpuManagerPolicy": "static",
  "cpuManagerReconcilePeriod": "5s",
  "topologyManagerPolicy": "best-effort",
...
   },
  "reservedSystemCPUs": "0-3,40-43",
  }
----

Additionally, the performanceprofile creates a “runTimeClass” that pods must specify within the pod.spec in order to fully achieve CPU isolation for the workload.

----
oc describe performanceprofile perf-profile-2m-worker
----

----
Name:         perf-profile-2m-worker
Namespace:
Labels:       <none>
Annotations:  <none>
API Version:  performance.openshift.io/v2
Kind:         PerformanceProfile
Spec:
 Cpu:
   Isolated:  4-39,44-79
   Reserved:  0-3,40-43
 Hugepages:
   Default Hugepages Size:  2M
   Pages:
     Count:  32000
     Node:   0
     Size:   2M
     Count:  32000
     Node:   1
     Size:   2M
 Node Selector:
   node-role.kubernetes.io/workerperf:
 Numa:
   Topology Policy:  best-effort
 Real Time Kernel:
   Enabled:  false
Status:
 Runtime Class:           performance-perf-profile-2m-worker
 Tuned:                   openshift-cluster-node-tuning-operator/openshift-node-performance-perf-profile-2m-worker
----

For workloads requiring CPU isolation in (OCP 4.7.11) the the pod.spec must have the following:

* For each container within the pod, resource requests and limits must be identical (Guaranteed Quality of Service)
* Request and Limits are in the form of whole CPUs
* The runTimeClassName must be specified
* Annotations disabling CPU and IRQ load-balancing

Example pod.spec:

----
metadata:
 annotations:
   cpu-load-balancing.crio.io: "disable"
   irq-load-balancing.crio.io: "disable"
 name: pao-example-podspec
spec:
 containers:
 - image: <PATH-TO-IMAGE>
   name: test
   resources:
     limits:
       cpu: 1
       memory: 1Gi
       hugepages-2Mi: 1000Mi s
     requests:
       cpu: 1
       memory: 1Gi
       hugepages-2Mi: 1000Mi
   restartPolicy: Always
   runtimeClassName: performance-perf-profile-2m-worker
----


.NUMA Awareness

Topology Manager collects hints from the CPU Manager, Device Manager, and other Hint Providers to align pod resources, such as CPU, SR-IOV VFs, and other device resources, for all Quality of Service (QoS) classes on the same non-uniform memory access (NUMA) node. This topology information and the configured Topology manager policy determine whether a workload is accepted or rejected on a node. (Note: To align CPU resources with other requested resources in a Pod spec, the CPU Manager must be enabled with the static CPU Manager policy.)

The following Topology manager policies are available and dependent on the requirements of the workload can be enabled. For high performance workloads making use of SR-IOV VFs, NUMA awareness follows the NUMA node to which the SR-IOV capable network adapter is connected.

*best-effort policy*

For each container in a pod with the best-effort topology management policy, kubelet calls each Hint Provider to discover their resource availability. Using this information, the Topology Manager stores the preferred NUMA Node affinity for that container. If the affinity is not preferred, Topology Manager stores this and admits the pod to the node.

*restricted policy*

For each container in a pod with the restricted topology management policy, kubelet calls each Hint Provider to discover their resource availability. Using this information, the Topology Manager stores the preferred NUMA Node affinity for that container. If the affinity is not preferred, Topology Manager rejects this pod from the node, resulting in a pod in a Terminated state with a pod admission failure.

*single-numa-node policy*

For each container in a pod with the single-numa-node topology management policy, kubelet calls each Hint Provider to discover their resource availability. Using this information, the Topology Manager determines if a single NUMA Node affinity is possible. If it is, the pod is admitted to the node. If a single NUMA Node affinity is not possible, the Topology Manager rejects the pod from the node. This results in a pod in a Terminated state with a pod admission failure.

For more information on Topology manager, see the following https://docs.openshift.com/container-platform/4.7/scalability_and_performance/using-topology-manager.html[[.underline]#OpenShift Documentation#].

