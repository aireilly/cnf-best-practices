// Metadata created by nebel
//
// ConvertedFromFile: cnf-reqs_1.3_single.adoc
// ConversionStatus: raw

[id="cnf-best-practices"]
= OpenShift Best Practices

.Logging

Log aggregation and analysis

The OpenShift platform will support logging from containers and forwarding those logs separately from the platform logging to a centralized logging repository. Logs may be forwarded based on Tenant Namespace identifier.

* Containers are expected to write logs to stdout
* Requires vendor to follow pod/container naming standards
* Logs will be forwarded to a centralized storage location
* Logs CAN be parsed so that specific vendor logs can be sent back to the CNF if required
* Requires vendor to provide svc/fqdn
* Logs may be sent back to the matching namespace using the below tag format
** vendor-function-000.logs ← Logs for namespace 000
** vendor-function-001.logs ← Logs for namespace 001
** Pod in the tenant namespace for receiving these logs:
*** Must use write any logs to a traditional log file on PV (disk) handling log rotation itself (either by using a framework or the traditional logrotate pattern)
*** Must NOT write anything to default *stdout*/*stderr* container pipes to avoid getting back into the log stream (avoiding a feedback loop), in other words that container must redirect *stdout*/*stderr* somewhere other than the default for the container

Log messages are aggregated as a JSON document after being normalized to add metadata. An example of a typical log message:

----
\{

"docker" : \{

"container_id" : "a2e6d10494f396a45e..."

},

"kubernetes" : \{

"container_name" : "rhel-logtest",

"namespace_name" : "logflatx",

"pod_name" : "rhel-logtest-987vr",

"container_image" : "quay.io/openshift/ocp-logtest:latest",

"container_image_id" : "docker.io/mffi….,

"pod_id" : "67667d28-13fe-4c89-aa44-06936279c399",

"host" : "ip-10-0-153-186.us-east-2.compute.internal",

"labels" : \{

"run" : "rhel-logtest",

"test" : "rhel-logtest"

},

"master_url" : "https://kubernetes.default.svc",

"namespace_id" : "e8fb5826-94f7-48a6-ae92-354e4b779008"

},

"message" : "2020-03-03 11:44:51,996 - SVTLogger - INFO",

"level" : "unknown",

"hostname" : "ip-10-0-153-186.us-east-2.compute.internal",

"pipeline_metadata" : \{

"collector" : \{

"ipaddr4" : "10.0.153.186",

"inputname" : "fluent-plugin-systemd",

"name" : "fluentd",

"received_at" : "2020-03-03T11:44:52.189331+00:00",

"version" : "1.7.4 1.6.0"

}

},

"@timestamp" : "2020-03-03T11:44:51.996384+00:00"

}
----

.Monitoring

Network Functions are expected to bring their own metrics collection functions (e.g. Prometheus) for their application specific metrics. This metrics collector will not be expected to nor be able to poll platform level metric data. Network Functions shall support exposing their Prometheus collection functions via PromQL interfaces to existing OSS systems.

Control Plane (infrastructure) metrics will be collected by the platform in a separate Prometheus instance.

.CPU allocation

It is important to note that when the OpenShift scheduler is placing pods, it first reviews the Pod CPU “Request” and schedules it if there is a node that meets the requirements. It will then impose the CPU “Limits” to ensure the Pod doesn’t consume more than the intended allocation. The limit can never be lower than the request.

NUMA Configuration

OpenShift provides a topology manager which leverages the CPU manager and Device manager to help associate processes to CPUs. Topology manager handles NUMA affinity. This feature is available as of OpenShift 4.6. For some examples on how to leverage the topology manager and creating workloads that work in real time, see the following links.

https://docs.openshift.com/container-platform/4.7/scalability_and_performance/using-topology-manager.html[[.underline]#https://docs.openshift.com/container-platform/4.7/scalability_and_performance/using-topology-manager.html#]

https://docs.openshift.com/container-platform/4.7/scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.html#performance-addon-operator-creating-workload-that-works-in-real-time_cnf-master[[.underline]#https://docs.openshift.com/container-platform/4.7/scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.html#performance-addon-operator-creating-workload-that-works-in-real-time_cnf-master#]

.Memory Allocation

Regarding memory allocation, there are a couple of considerations. How much of the platform is OpenShift itself using, and how much is left over to allocate for the applications running on OpenShift.

Once it has been determined how much memory is left over for the applications, quotas can be applied which specify both the requested amount of memory and limits. In the case of where a memory request has been specified, OpenShift will not schedule the pod unless the amount of memory required to launch it is available. In the case of a limit being specified, OpenShift will not allocate more memory to the application than the limit provides. It is important to note that when the OpenShift scheduler is placing pods, it first reviews the Pod memory “Request” and schedules it if there is a node that meets the requirements. It will then impose the memory “Limits” to ensure the Pod doesn’t consume more than the intended allocation. The limit can never be lower than the request.

Vendors must supply quotas https://docs.openshift.com/container-platform/4.4/applications/quotas/quotas-setting-per-project.html[[.underline]#per project#] so that nodes can be sized appropriately and clusters are able to support the needs of vendor applications.

.Affinity/Anti-affinity

In OpenShift Container Platform pod affinity and pod anti-affinity allow you to constrain which nodes your pod is eligible to be scheduled on based on the key/value labels on other pods. There are two types of affinity rules, required and preferred. Required rules must be met, whereas preferred rules are best effort.

These pod affinity / anti-affinity rules are set in the pod specification as matchExpressions to a labelSelector. See the following link for examples and more information. See the following example for more information here:

----
apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
	podAffinity:
  	requiredDuringSchedulingIgnoredDuringExecution:
  	- labelSelector:
      	matchExpressions:
      	- key: security
        	operator: In
        	values:
        	- S1
    	topologyKey: failure-domain.beta.kubernetes.io/zone
  containers:
  - name: with-pod-affinity
	image: quay.io/ocpqe/hello-pod
----

https://docs.openshift.com/container-platform/4.7/nodes/scheduling/nodes-scheduler-pod-affinity.html#nodes-scheduler-pod-affinity[[.underline]#https://docs.openshift.com/container-platform/4.7/nodes/scheduling/nodes-scheduler-pod-affinity.html#nodes-scheduler-pod-affinity#]

.Taints and Tolerations

Taints and tolerations allow the Node to control which Pods should (or should not) be scheduled on them. A taint allows a node to refuse a pod to be scheduled unless that pod has a matching toleration.

You apply taints to a node through the node specification (NodeSpec) and apply tolerations to a pod through the pod specification (PodSpec). A taint on a node instructs the node to repel all pods that do not tolerate the taint.

Taints and tolerations consist of a key, value, and effect. An operator allows you to leave one of these parameters empty.

It is possible to utilize taints and tolerations to allow pods to be rescheduled and moved from nodes that are in need of maintenance. Pods may be forcibly ejected from nodes to perform necessary maintenance. Do apply tolerations for NoExecute, PreferNoSchedule, and NoSchedule.

See https://docs.openshift.com/container-platform/4.7/nodes/scheduling/nodes-scheduler-taints-tolerations.html[[.underline]#https://docs.openshift.com/container-platform/4.7/nodes/scheduling/nodes-scheduler-taints-tolerations.html#] for more information.

.Requests/Limits

Requests and limits provide a way for a CNF developer to ensure they have adequate resources available to run the application. Requests can be made for storage, memory, CPU and so on. These requests and limits can be enforced by quotas. The production platform may utilize quotas as a way to enforce requests and limits. See the following for more information.

https://docs.openshift.com/container-platform/4.7/applications/quotas/quotas-setting-per-project.html[[.underline]#https://docs.openshift.com/container-platform/4.7/applications/quotas/quotas-setting-per-project.html#]

It is possible to overcommit node resources in development environments.

Keep in mind though, that a node can be overcommitted which can affect the strategy of request / limit implementation. For example, when you need guaranteed capacity, use quotas to enforce and in a development environment, you can overcommit where a trade-off of guaranteed performance for capacity is acceptable. Overcommitment can be done on a project, node or cluster level.

https://docs.openshift.com/container-platform/4.7/nodes/clusters/nodes-cluster-overcommit.html[[.underline]#https://docs.openshift.com/container-platform/4.7/nodes/clusters/nodes-cluster-overcommit.html#]

.Pods

.No naked pods

Do not use naked Pods (that is, Pods not bound to a ReplicaSet or Deployment). Naked Pods will not be rescheduled in the event of a node failure.

.Image tagging

An image tag is a label applied to a container image in a repository that distinguishes a specific image from other images. Image tags may be used to categorize images as latest, stable, development and by versions within those two categories. This allows the administrator to be specific when declaring which image to test, or which image to run in production.

link:https://docs.openshift.com/container-platform/4.7/openshift_images/managing_images/tagging-images.html[]

.One process per container

OpenShift organizes workloads into pods. Pods are the smallest unit of a workload that Kubernetes understands. Within pods, one can have one or more containers. Containers are essentially composed of the runtime that is required to launch and run a process.

Each container should run only one process. Different processes should always be split between containers, and where possible also separate into different pods. This can help in a number of ways, such as troubleshooting, upgrades and more efficient scaling.

However, OpenShift does support running multiple containers per pod. This can be useful if parts of the application need to share namespaces like networking and storage resources. Additionally, there are other models like launching init containers, sidecar containers, etc. which may justify running multiple containers in a single pod.

Applications that utilize service mesh will have an additional container injected into their pods to proxy workload traffic.

More information about pods can be found here.

https://docs.openshift.com/container-platform/4.7/nodes/pods/nodes-pods-using.html[[.underline]#https://docs.openshift.com/container-platform/4.7/nodes/pods/nodes-pods-using.html#]

.init containers

Init containers can be used for running tools / commands / or any other action that needs to be done before the actual pod is started. For example, loading a database schema, or constructing a config file from a definition passed in via configMap or secret.

https://docs.openshift.com/container-platform/4.7/nodes/containers/nodes-containers-init.html[[.underline]#https://docs.openshift.com/container-platform/4.7/nodes/containers/nodes-containers-init.html#]

.Security/RBAC

Roles / RolesBinding - A Role represents a set of permissions within a particular namespace. E.g: A given user can list pods/services within the namespace. The RoleBinding is used for granting the permissions defined in a role to a user or group of users.

ClusterRole / ClusterRoleBinding - A ClusterRole represents a set of permissions at the Cluster level. E.g: A given user has “cluster-admin” privileges on the cluster. The ClusterRoleBinding is used for granting the permissions defined in a ClusterRole to a user or group of users.

https://docs.openshift.com/container-platform/4.7/authentication/using-rbac.html[[.underline]#https://docs.openshift.com/container-platform/4.7/authentication/using-rbac.html#]

.Multus

Multus is a meta CNI that allows multiple CNIs that it delegates to. This allows pods to get additional interfaces beyond eth0 via additional CNIs. The solution may have additional CNIs for SR-IOV and MacVLAN interfaces. This would allow for direct routing of traffic to a pod without using the pod network via additional interfaces. This capability is being delivered for use in only corner case scenarios, it is not to be used in general for all applications. Example use cases include bandwidth requirements that necessitate SR-IOV and protocols that are unable to be supported by the load balancer. The OVN based pod network should be used for every interface that can be supported from a technical standpoint.

https://docs.openshift.com/container-platform/4.7/networking/multiple_networks/understanding-multiple-networks.html[[.underline]#https://docs.openshift.com/container-platform/4.7/networking/multiple-networks/understanding-multiple-networks.html#]

.Multus SR-IOV / MACVLAN

SR-IOV is a specification that allows a PCIe device to appear to be multiple separate physical PCIe devices. The Performance Addon component allows you to validate SR-IOV by running DPDK, SCTP and device checking tests.

SR-IOV and MACVLAN interfaces are able to be requested for protocols that do not work with the default CNI or for exceptions where a network function has not been able to move functionality onto the CNI. These are exception use cases. Multus interfaces will be defined by the platform operations team for the network functions which can then consume them. Multus interfaces will have to be requested via the planning tools ahead of time by the companies personnel. VLANs will be applied by the SR-IOV VF, thus the VLAN/network that the SR-IOV interface requires must be part of the request for the namespace.

https://docs.openshift.com/container-platform/4.7/networking/hardware_networks/about-sriov.html[[.underline]#https://docs.openshift.com/container-platform/4.7/networking/hardware_networks/about-sriov.html#]

By configuring the SR-IOV Network CRs named NetworkAttachmentDefinitions are exposed by the SR-IOV Operator in the CNF namespace.

Different names will be assigned to different Network Attachment Definitions that are namespace specific. MACVLAN versus Multus interfaces will be named differently to distinguish the type of device assigned to them (created by configuring SR-IOV devices via the SriovNetworkNodePolicy CR).

From the CNF perspective, a defined set of network attachment definitions will be available in the assigned namespace to serve secondary networks for regular usage or to serve for DPDK payloads.

The SR-IOV devices are configured by the cluster admin, and they will be available in the namespace assigned to the CNF. The command “oc -n <cnfnamespace> get network-attachment-definitions” will return the list of secondary networks available in the namespace.

.SR-IOV Interfaces settings

The following settings must be negotiated with the cluster administrator, for each network type available in the namespace:

* The type of netdevice to be used for the VF (kernel or userspace)
* The vlan ID to be applied to a given set of VFs available in a namespace
* For kernel-space devices, the ip allocation is provided directly by the cluster ip assignment mechanism.
* The option to configure the ip of a given SR-IOV interface at runtime (see https://docs.openshift.com/container-platform/4.7/networking/hardware_networks/add-pod.html#runtime-config-ethernet_configuring-sr-iov[[.underline]#https://docs.openshift.com/container-platform/4.7/networking/hardware_networks/add-pod.html#runtime-config-ethernet_configuring-sr-iov#]).

Example sriovnetworknodepolicy: NOTE: this is enabled by the cluster administrator.

----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
 name: nnp-w1ens3f0grp2
 namespace: openshift-sriov-network-operator
spec:
 deviceType: vfio-pci
 isRdma: false
 linkType: eth
 mtu: 9000
 nicSelector:
   deviceID: 158b
   pfNames:
   - ens3f0#50-63
   vendor: "8086"
 nodeSelector:
   kubernetes.io/hostname: worker-3
 numVfs: 64
 priority: 99
 resourceName: w1ens3f0grp2
----

The sriovnetwork CR creates the network-attach-definition within the target networkNamespace

.Example 1: Empty IPAM
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: sriovnet
  namespace: openshift-sriov-network-operator
spec:
  capabilities: '{ "mac": true }'
  ipam: '{}'
  networkNamespace: <CNF-NAMESPACE>
  resourceName: w1ens3f0grp2
  spoofChk: "off"
  trust: "on"
  vlan: 282
----

.Example 2: Whereabouts IPAM
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: sriovnet
  namespace: openshift-sriov-network-operator
spec:
  capabilities: '{ "mac": true }'
  ipam: '{"type":"whereabouts","range":"FD97:0EF5:45A5:4000:00D0:0403:0000:0001/64","range_start":"FD97:0EF5:45A5:4000:00D0:0403:0000:0001","range_end":"FD97:0EF5:45A5:4000:00D0:0403:0000:0020","routes":[{"dst":"fd97:0ef5:45a5::/48","gw":"FD97:EF5:45A5:4000::1"}]}'
  networkNamespace: <CNF-NAMESPACE>
  resourceName: w1ens3f0grp2
  spoofChk: "off"
  trust: "on"
----

*vlan: 282*

.Example 3: Static IPAM
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: sriovnet
  namespace: openshift-sriov-network-operator
spec:
  capabilities: '{ "mac": true }'
  ipam: '{"type": "static","addresses":[{"address":"10.120.26.5/25","gateway":"10.120.26.1"}]}'
  networkNamespace: <CNF-NAMESPACE>
  resourceName: w1ens3f0grp2
  spoofChk: "off"
  trust: "on"
  vlan: 282
----


.Example 4: Using Pod Annotations to attach
----
apiVersion: v1
kind: Pod
metadata:
  name: sample-pod
  annotations:
    k8s.v1.cni.cncf.io/networks: |-
      [
        {
          "name": "net1",
          "mac": "20:04:0f:f1:88:01",
          "ips": ["192.168.10.1/24", "2001::1/64"]
        }
----

The examples depict scenarios used within the Core solution to deliver secondary network interfaces with and without IPAM to a pod.

Example 1, creates a network attachment definition that does not specify an IP address, example 2 makes use of the static IPAM and example 3 makes use of the whereabouts CNI that provides a cluster wide dhcp option.

The actual addresses used for both whereabouts and static IPAM are managed external to the cluster.

The above Sriovnetwork CR will configure a network attachment definition within the CNF’s namespace.

----
[c]$ oc get net-attach-def -n <CNF-NAMESPACE>
NAME             AGE
sriovnet        9d
----

Within the CNF namespace the sriov resource is consumed via a pod annotation:

----
kind: Pod
metadata:
 annotations:
   k8s.v1.cni.cncf.io/networks: sriovnet
----

.Attaching the VF to a pod

Once the right network attachment definition is found, applying the k8s.v1.cni.cncf.io/networks annotation with the name of the network attachment definition to the pod will add the additional network interfaces in the pod namespace, as per the following example:

.Example
----
apiVersion: v1
kind: Pod
metadata:
  name: sample-pod
  annotations:
    k8s.v1.cni.cncf.io/networks: |-
      [
        {
          "name": "net1",
          "mac": "20:04:0f:f1:88:01",
          "ips": ["192.168.10.1/24", "2001::1/64"]
        }
      ]
----

.Discovering SR-IOV devices properties from the application

All the properties of the interfaces are added to the pod’s _k8s.v1.cni.cncf.io/network-status_ annotation. The annotation is json-formatted and for each network object contains information such as ips (where available), mac address, pci address.

_For example:_

----
k8s.v1.cni.cncf.io/network-status: |-
  	[{
      	"name": "",
      	"interface": "eth0",
      	"ips": [
          	"10.132.3.148"
      	],
      	"mac": "0a:58:0a:84:03:94",
      	"default": true,
      	"dns": {}
  	},{
      	"name": "cnfns/networkname",
      	"interface": "net1",
      	"ips": [
          	"1.1.1.2"
      	],
      	"mac": "ba:1d:e7:31:2a:e0",
      	"dns": {},
      	"device-info": {
          	"type": "pci",
          	"version": "1.0.0",
          	"pci": {
              	"pci-address": "0000:19:00.5"
          	}
      	}
  	}]
----

Note: the ip information is not available if the driver specified is vf-io.

The same annotation is available as a file content inside the pod, at the /etc/podnetinfo/annotations path. A convenience library is available to easily consume those informations from the application (bindings in C and Go https://docs.openshift.com/container-platform/4.7/networking/hardware_networks/about-sriov.html#nw-sriov-app-netutil_about-sriov).

.NUMA awareness

If the pod is using a guaranteed QoS class and the kubelet is configured with a suitable topology manager policy (restricted, single-numa node) then the VF assigned to the pod will belong to the same NUMA node as the other assigned resources (CPU and other NUMA aware devices).

See the Performance Add-On Operator section for NUMA awareness and more information about how HugePages are turned on.

.Upgrades

.Handling platform upgrades

* CNF vendors should expect that the platform may be upgraded to new versions on an ongoing basis employing CI/CD runtime deployment without any advance notice to CNF vendors.
* During platform upgrades, the Kubernetes API deprecation policy defined in https://kubernetes.io/docs/reference/using-api/deprecation-policy/[[.underline]#https://kubernetes.io/docs/reference/using-api/deprecation-policy/#] shall be followed
* CNFs are expected to maintain service continuity during Platform Upgrades, and during CNF version upgrades
* CNFs need to be prepared for nodes to reboot or shut down without notice.
* CNFs shall configure pod disruption budget appropriately to maintain service continuity during platform upgrades
* Applications *may NOT* deploy pod disruption budgets that prevent zero pod disruption
* Applications should not be tied to a specific version of Kubernetes or any of its components

.OpenShift Virtualization/kubevirt

.OpenShift Virtualization and VMs (CNV) best practices

The platform was designed as a pure container-based system, where all network functions are containerized. However, it has become apparent that some NFs have not completed re-architecting all components of their network functions to be fully containerized. In order to deal with this lag, VMs are orchestrated via Kubernetes for an interim period of time for applications that require low latency connectivity between containers and these VMs. When OpenShift Virtualization becomes generally-available for enterprise workloads, such throughput- and latency-insensitive workloads may be added to the cluster. VNFs and other throughput- or latency-sensitive applications can be considered only after careful validation. Until then, it is recommended to keep these workloads on OSP VMs.

OpenShift Virtualization should be installed according to its documentation, and only documented supported features may be used unless an explicit exception has been granted.

See: link:https://docs.openshift.com/container-platform/4.7/virt/about-virt.html[]

In order to improve overall virtualization performance and reduce CPU latency, critical VNFs can take advantage of OpenShift Virtualization's high-performance features. These can provide the VNFs with dedicated physical CPUs[1] and "isolate" QEMU threads, such as the emulator thread and the IO thread, on a separate physical CPU[2][3] so it will not affect the workloads CPU latency.

Similar to OpenStack, OpenShift Virtualization supports the device role tagging mechanism[4]. for the network interfaces (same format as it is in OSP). Users will be able to tag Network interfaces in the API and identify them in device metadata provided to the guest OS via the config drive.

[1] link:https://docs.openshift.com/container-platform/4.7/virt/virtual_machines/advanced_vm_management/virt-dedicated-resources-vm.html[]

[2] link:https://kubevirt.io/user-guide/virtual_machines/dedicated_cpu_resources/#requesting-dedicated-cpu-for-qemu-emulator[]

[3] link:https://kubevirt.io/user-guide/#/creation/disks-and-volumes?id=iothreads-with-qemu-emulator-thread-and-dedicated-pinned-cpus[]

[4] link:https://kubevirt.io/user-guide/virtual_machines/startup_scripts/#device-role-tagging[]

.VM Image Import Recommendations (CDI)

OpenShift Virtualization VMs store their persistent disks on kubernetes Persistent Volumes (PVs). PVs are requested by VMs using kubernetes Persistent Volume Claims (PVCs). VMs may require a combination of blank and pre-populated disks in order to function. Blank disks can be initialized automatically by kubevirt when an empty PV is initially encountered by a starting VM. Other disks must be populated prior to starting the VM. OpenShift Virtualization provides a component called the Containerized Data Importer (CDI) which automates the preparation of pre-populated persistent disks for VMs. CDI integrates with KubeVirt to synchronize VM creation and deletion with disk preparation by using a custom resource called a DataVolume. Using DataVolumes, data can be imported into a PV from various sources including container registries and HTTP servers.

The following recommendations should be followed when managing persistent disks for VMs:

* Blank disks: Create a PVC and associate it with the VM using a persistentVolumeClaim volume type in the volumes section of the VirtualMachine spec.
* Populated disks: In the VirtualMachine spec, add a DataVolume to the dataVolumeTemplates section and always use the dataVolume volume type in the volumes section.

.Working with large VM disk images

In contrast to container images, VM disk images can be quite large (30GiB or more is common). It is important to consider the costs of transferring large amounts of data when planning workflows involving the creation of VMs (especially when scaling up the number of VMs). The efficiency of an image import depends on the format of the file and also the transfer method used. The most efficient workflow, for two reasons, is to host a gzip-compressed raw image on a server and import via HTTP. Compression avoids transferring zeros present in the free space of the image, and CDI can stream the contents directly into the target PV without any intermediate conversion steps. In contrast, images imported from a container registry must be transferred, unarchived, and converted prior to being usable. These additional steps increase the amount of data transferred between a node and the remote storage.

