// Metadata created by nebel
//
// ConvertedFromFile: cnf-reqs_1.3_single.adoc
// ConversionStatus: raw

[id="cnf-dev-guidelines"]
= CNF Developer Guidelines

.Preface

Cloud-native Network Functions (CNFs) are containerized instances of classic physical or virtual network functions (VNF) which have been decomposed into microservices supporting elasticity, lifecycle management, security, logging, and other capabilities in a Cloud-Native format.

.Goal

This section is mainly for the developers of CNFs, who need to build high-performance Network Functions in a containerized environment. The guidance provided in this guide should assist partners when developing their CNFs so that they can be deployed on the OpenShift Container Platform (OCP) in a secure, efficient and supportable way.

.Non-Goal

This is not a guide on how to build CNF’s functionality.

.Principle of Least Privilege

In OpenShift Container Platform (OCP), it is possible to run privileged containers that have all of the root capabilities on a host machine, allowing the ability to access resources which are not accessible in ordinary containers. This, however, increases the security risk to the whole cluster. Containers should only request those privileges they need to run their legitimate functions. No containers will be allowed to run with full privileges without an exception.

The general guidelines are:

. Only ask for the necessary privileges and access control settings for your application.
. If the function required by your CNF can be fulfilled by OCP components, your application should not be requesting escalated privilege to perform this function.
. Avoid using any host system resource if possible.
. Leveraging read only root filesystem when possible.

.Avoid Accessing Resources on Host

It is not recommended for an application to access the following resources on the host.

.Avoid Mounting host directories as volumes

It is not necessary to mount host /sys/ or host /dev/ directory as a volume in a pod in order to use a network device such as SR-IOV VF. The moving of a network interface into the pod network namespace is done automatically by CNI. Mounting the whole /sys/ or /dev/ directory in the container will overwrite the network device descriptor inside the container which causes 'device not found' or 'no such file or directory' error.

Network interface statistics can be queried inside the container using the same /sys/ path as was done when running directly on the host. When running network interfaces in containers, relevant /sys/ statistics interfaces are available inside the container, such as '/sys/class/net/net1/statistics/', '/proc/net/tcp' and '/proc/net/tcp6'.

For running DPDK applications with SR-IOV VF, device specs (in case of vfio-pci) are automatically attached to the container via the Device Plugin. There is no need to mount the /dev/ directory as a volume in the container as the application can find device specs under '/dev/vfio/' in the container.

.Avoid the host’s network namespace

Application pods must avoid using hostNetwork. Applications may not use the host network, including nodePort for network communication. Any networking needs beyond the functions provided by the pod network and ingress/egress proxy must be serviced via a multus connected interface.

.Capabilities

Linux Capabilities allow you to break apart the power of root into smaller groups of privileges.

The Linux capabilities(7) man page provides a detailed description of how capabilities management is performed in Linux. In brief the Linux kernel associates various capability sets with threads and files. The thread’s Effective capability set determines the current privileges of a thread. When a thread executes a binary program the kernel updates the various thread capability sets according to a set of rules that take into account the UID of the thread before and after the exec system call and the file capabilities of the program being executed. Refer to the blog series in link:#kix.m02gjmmhqh0e[[.underline]#[10]#] for more details about Linux capabilities and some examples.

Users may choose to specify the required permissions for their running application in the Security Context of the pod specification. In OCP, administrators can use the Security Context Constraint (SCC) admission controller plugin to control the permissions allowed for pods deployed to the cluster. If the pod requests permissions that are not allowed by the SCCs available to that pod, the pod will not be admitted to the cluster.

The following runtime and SCC attributes control the capabilities that will be granted to a new container:

* The capabilities granted to the CRI-O engine. The default capabilities are listed here: https://github.com/cri-o/cri-o/blob/master/internal/config/capabilities/capabilities.go +
Note that as of version 1.18, CRI-O no longer runs with NET_RAW or SYS_CHROOT by default. https://cri-o.github.io/cri-o/v1.18.0.html
* The values in the SCC for allowedCapabilities, defaultAddCapabilities and requiredDropCapabilities
* allowPrivilegeEscalation: controls whether a container can acquire extra privileges through setuid binaries or the file capabilities of binaries

The capabilities associated with a new container are determined as follows:

* If the container has the UID 0 (root) its Effective capability set is determined according to the capability attributes requested by the pod or container security context and allowed by the SCC assigned to the pod. In this case, the SCC provides a way to limit the capabilities of a root container.
* If the container has a UID non 0 (non root), the new container has an empty Effective capability set (see https://github.com/kubernetes/kubernetes/issues/56374[[.underline]#https://github.com/kubernetes/kubernetes/issues/56374#]). In this case the SCC assigned to the pod controls only the capabilities the container may acquire through the file capabilities of binaries it will execute.

Considering the general recommendation to avoid running root containers, capabilities required by non-root containers are controlled by the pod or container security context and the SCC capability attributes but can only be acquired by properly setting the file capabilities of the container binaries.

Refer to link:#kix.h926go207d3h[[.underline]#[1]#] for more details on how to define and use the SCC.

.DEFAULT Capabilities

The default capabilities that are allowed via the restricted SCC are as follows.

https://github.com/cri-o/cri-o/blob/master/internal/config/capabilities/capabilities.go[[.underline]#https://github.com/cri-o/cri-o/blob/master/internal/config/capabilities/capabilities.go#]

"CHOWN",

"DAC_OVERRIDE",

"FSETID",

"FOWNER",

"SETPCAP",

"NET_BIND_SERVICE"

.IPC_LOCK

IPC_LOCK capability is required if any of these functions are used in an application:

* mlock()
* mlockall()
* shmctl()
* mmap().

Even though ‘mlock’ is not necessary on systems where page swap is disabled (for example on OpenShift), it may still be required as it is a function that is built into DPDK libraries, and DPDK based applications may indirectly call it by calling other functions.

.NET_ADMIN

NET_ADMIN capability is required to perform various network related administrative operations inside container such as:

* MTU setting
* Link state modification
* MAC/IP address assignment
* IP address flushing
* Route insertion/deletion/replacement
* Control network driver and hardware settings via ‘ethtool’

This doesn't include:

* add/delete a virtual interface inside a container. For example: adding a VLAN interface
* Setting VF device properties

All the administrative operations (except 'ethtool') mentioned above that require the NET_ADMIN capability should already be supported on the host by various CNIs in Openshift.

.(Avoid) SYS_ADMIN

This capability is very powerful and overloaded. It allows the application to perform a range of system administration operations to the host. So you should avoid requiring this capability in your application.

.SYS_NICE

In the case that a CNF is running on a node using the real-time kernel, SYS_NICE will be used to allow real-time application to switch to SCHED_FIFO.

.SYS_PTRACE

This capability is required when using Process Namespace Sharing. This is used when processes from one Container need to be exposed to another Container. For example, to send signals like SIGHUP from a process in a Container to another process in another Container. See link:#kix.x9sn5ltpet8c[[.underline]#[9]#] for more details

.Operations that shall be executed by OpenShift

The application should not require NET_ADMIN capability to perform the following administrative operations:

* MTU setting
** For the cluster network, also known as the OVN or Openshift-SDN network, the MTU shall be configured by modifying the manifests generated by openshift-installer before deploying the cluster. Refer to link:#kix.1mwd3pp1xi0x[[.underline]#[4]#] for more information.
** For the additional networks managed by the Cluster Network Operator, it can be configured through the NetworkAttachmentDefinition resources generated by the Cluster Network Operator. Refer to link:#kix.hutwtlst36x3[[.underline]#[5]#] for more information.
** For the SRIOV interfaces managed by the Sriov Network Operator. Refer to link:#kix.prsfggqin4x0[[.underline]#[6]#] for more information.
* Link state modification
** All the links will be set to up before attaching it to a pod.
* IP/MAC address assignment
** For all the networks, the IP/MAC address will be assigned to the interface during pod creation.
** Multus also allows users to override the IP/MAC address. Refer to link:#kix.7snexvyvu78o[[.underline]#[7]#] for more information.
* Manipulate pod’s route table
** By default, the default route of the pod will point to the cluster network, with or without the additional networks. Multus also allows users to override the default route of the pod. Refer to link:#kix.7snexvyvu78o[[.underline]#[7]#] for more information.
** Non-default routes can be added to pod’s routing table by various IPAM CNI plugins during pod creation
* SRIOV VF setting
** Besides the functions aforementioned, the SRIOV Network Operator supports configuring the following parameters for SRIOV VFs. Refer to link:#kix.jowj8zhw9jik[[.underline]#[8]#] for more information.
*** vlan
*** linkState
*** maxTxRate
*** minTxRate
*** vlanQoS
*** spoofChk
*** trust
* Multicast
** In OCP, multicast is supported for both the default interface (OVN or OpenShift-SDN) and the additional interfaces (macvlan, SR-IOV...). However, multicast is disabled by default. To enable it please refer to link:#kix.ekb08grpyt9o[[.underline]#[2]#] link:#kix.v13xkav9etjm[[.underline]#[3]#].
** If your application works as a multicast source and you want to utilize the additional interfaces to carry the multicast traffic, then you don’t need the NET_ADMIN capability. But you need to follow the instruction in link:#kix.v13xkav9etjm[[.underline]#[3]#] to set the correct multicast route in your pod’s routing table.

.Operations that can NOT be executed by OpenShift

All the CNI plugins will only be invoked during pod creation and deletion. If your CNF wants to perform any operations mentioned in the above chapter at runtime, the NET_ADMIN capability would be required. There are some other functionalities that are not currently supported by any of the OpenShift components which would also require NET_ADMIN capability:

* Link state modification at runtime
* IP/MAC modification at runtime
* Manipulate pod’s route table or firewall rules at runtime
* SRIOV VF setting at runtime
* Netlink configuration
** For example, ‘ethtool’ can be used to configure things like rxvlan, txvlan, gso, tso, etc.
* Multicast
** If your application works as a receiving member of IGMP groups, you need to specify the NET_ADMIN capability in the pod manifest. So that the app is allowed to assign multicast addresses to the pod interface and join an IGMP group.
* Set SO_PRIORITY to a socket to manipulate the 802.1p priority in ethernet frames
* Set IP_TOS to a socket to manipulate the DSCP value of IP packets

.Analyzing Your Application

To find out which capabilities the application needs, Red Hat developed a SystemTap script (container_check.stp). With this tool, the CNF developer can find out what capabilities an application requires in order to run in a container. It also shows the syscalls which were invoked.

Another tool is ‘capable’ which is part of the BCC tools. It can be installed on RHEL8 with “dnf install bcc”.

.Example

Here is an example of how to find out the capabilities that an application needs. ‘testpmd’ is a DPDK based layer-2 forwarding application. It needs the CAP_IPC_LOCK to allocate the hugepage memory.

. Use container_check.stp. We can see CAP_IPC_LOCK and CAP_SYS_RAWIO are requested by ‘testpmd’ and the relevant syscalls.
+
----
$ /usr/share/systemtap/examples/profiling/container_check.stp -c 'testpmd -l 1-2 -w 0000:00:09.0 -- -a --portmask=0x8 --nb-cores=1'
[...]
capabilities used by executables
      executable:      prob capability

         testpmd:         cap_ipc_lock
         testpmd:        cap_sys_rawio



capabilities used by syscalls
      executable,              syscall (       capability ) :            count
         testpmd,             mlockall (     cap_ipc_lock ) :                1
         testpmd,                 mmap (     cap_ipc_lock ) :              710
         testpmd,                 open (    cap_sys_rawio ) :                1
         testpmd,                 iopl (    cap_sys_rawio ) :                1


forbidden syscalls
      executable,              syscall:            count


failed syscalls
      executable,              syscall =            errno:            count
 eal-intr-thread,           epoll_wait =            EINTR:                1
   lcore-slave-2,                 read =                 :                1
   rte_mp_handle,              recvmsg =                 :                1
          stapio,                      =            EINTR:                1
          stapio,               execve =           ENOENT:                3
          stapio,        rt_sigsuspend =                 :                1
         testpmd,                flock =           EAGAIN:                5
         testpmd,                 stat =           ENOENT:               10
         testpmd,                mkdir =           EEXIST:                2
         testpmd,             readlink =           ENOENT:                3
         testpmd,               access =           ENOENT:             1141
         testpmd,               openat =           ENOENT:                1
         testpmd,                 open =           ENOENT:               13
[...]
----
. Use capable command
+
----
$ /usr/share/bcc/tools/capable
----
. Start the testpmd application from another terminal, and send some test traffic to it.
+
----
$ testpmd -l 18-19 -w 0000:01:00.0 -- -a --portmask=0x1 --nb-cores=1
----
. Check the output of the ‘capable’ command. As we can see CAP_IPC_LOCK was requested for running ‘testpmd’.
+
----
[...]
00:41:58  0      3591   3591   testpmd          14   CAP_IPC_LOCK         1
00:41:58  0      3591   3591   testpmd          14   CAP_IPC_LOCK         1
00:41:58  0      3591   3591   testpmd          14   CAP_IPC_LOCK         1
00:41:58  0      3591   3591   testpmd          14   CAP_IPC_LOCK         1
00:41:58  0      3591   3591   testpmd          14   CAP_IPC_LOCK         1
00:41:58  0      3591   3591   testpmd          14   CAP_IPC_LOCK         1
00:41:58  0      3591   3591   testpmd          14   CAP_IPC_LOCK         1
00:41:58  0      3591   3591   testpmd          14   CAP_IPC_LOCK         1
00:41:58  0      3591   3591   testpmd          14   CAP_IPC_LOCK         1
00:41:58  0      3591   3591   testpmd          14   CAP_IPC_LOCK         1
00:41:58  0      3591   3591   testpmd          14   CAP_IPC_LOCK         1
00:41:58  0      3591   3591   testpmd          14   CAP_IPC_LOCK         1
00:41:58  0      3591   3591   testpmd          14   CAP_IPC_LOCK         1
[...]
----
. Also, we can try to run ‘testpmd’ without the CAP_IPC_LOCK with ‘capsh’. Now we can see that the hugepage memory cannot be allocated.
+
----
$ capsh --drop=cap_ipc_lock -- -c testpmd -l 18-19 -w 0000:01:00.0 -- -a --portmask=0x1 --nb-cores=1
EAL: Detected 24 lcore(s)
EAL: Detected 2 NUMA nodes
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: No free hugepages reported in hugepages-1048576kB
EAL: Probing VFIO support...
EAL: VFIO support initialized
EAL: PCI device 0000:01:00.0 on NUMA socket 0
EAL:   probe driver: 8086:10fb net_ixgbe
EAL:   using IOMMU type 1 (Type 1)
EAL: Ignore mapping IO port bar(2)
EAL: PCI device 0000:01:00.1 on NUMA socket 0
EAL:   probe driver: 8086:10fb net_ixgbe
EAL: PCI device 0000:07:00.0 on NUMA socket 0
EAL:   probe driver: 8086:1521 net_e1000_igb
EAL: PCI device 0000:07:00.1 on NUMA socket 0
EAL:   probe driver: 8086:1521 net_e1000_igb
EAL:   cannot set up DMA remapping, error 12 (Cannot allocate memory)
testpmd: mlockall() failed with error "Cannot allocate memory"
testpmd: create a new mbuf pool <mbuf_pool_socket_0>: n=331456, size=2176, socket=0
testpmd: preferred mempool ops selected: ring_mp_mc
EAL:   cannot set up DMA remapping, error 12 (Cannot allocate memory)
testpmd: create a new mbuf pool <mbuf_pool_socket_1>: n=331456, size=2176, socket=1
testpmd: preferred mempool ops selected: ring_mp_mc
EAL:   cannot set up DMA remapping, error 12 (Cannot allocate memory)
EAL:   cannot set up DMA remapping, error 12 (Cannot allocate memory)
----

.CNF Best Practice

The design and implementation of CNFs may be varied. However, from the platform networking perspective we can put them into the following categories. Here we have some recommendations for each kind of application on what capabilities it shall request.

.Control Plane and Management CNFs

[width="100%",cols="51%,49%",options="header",]
|===
|Vocabulary |
a|
CNF

a|
Cloud-native Network Function

a|
CNI

a|
Container Network Interface

a|
DPDK

a|
Data Plane Development Kit

a|
DSCP

a|
Differentiated Services Code Point

a|
IP

a|
Internet Protocol

a|
MTU

a|
Maximum Transmission Unit

a|
OVN

a|
Open Virtual Network

a|
PF

a|
Physical Function

a|
PMD

a|
Poll Mode Driver

a|
QoS

a|
Quality of Service

a|
RHEL

a|
Red Hat Enterprise Linux

a|
SR-IOV

a|
Single Root I/O Virtualization

a|
VLAN

a|
Virtual Local Area Network

a|
VF

a|
Virtual Function

a|
VPP

a|
Vector Packet Processor

|===

