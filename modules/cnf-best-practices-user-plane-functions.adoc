[id="cnf-best-practices-user-plane-functions"]
= User Plane Functions

== Performance Addon Operator (PAO)

Red Hat created the link:Performance Addon Operator# for low latency nodes. The emergence of Edge computing []in the area of Telco / 5G plays a key role in reducing latency and congestion problems andimproving application performance. Many of the deployed applications in the Telco space require low latency that can only tolerate zero packet loss. OpenShift Container Platform provides a Performance Addon Operator to implement automatic tuning to achieve low latency performance for applications. The PAO is a meta operator that leverages MachineConfig, Topology Manager, CPU Manager, Tuned and KubeletConfig to optimize the nodes.

== Hugepages

In the Openshift Container Platform, nodes/hosts must pre-allocate huge pages.

For further reading on OCP's support of huge pages, see the Configuring huge pages documentation below.

link:https://docs.openshift.com/container-platform/4.7/scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.html#cnf-configuring-huge-pages_cnf-master[]

== CPU Isolation

The Performance Addon Operator manages host CPUs by dividing them into reserved CPUs for cluster and operating system housekeeping duties, and isolated CPUs for workloads. CPUs that are used for low latency workloads are set as isolated.

Device interrupts are load balanced between all isolated and reserved CPUs to avoid CPUs being overloaded, with the exception of CPUs where there is a guaranteed pod running. Guaranteed pod CPUs are prevented from processing device interrupts when the relevant annotations are set for the pod.

.CNF Requirement
[IMPORTANT]
====
In order to use Isolated CPUs, specific annotations must be defined in the pod spec
====

== Topology Manager and NUMA Awareness

Topology Manager collects hints from the CPU Manager, Device Manager, and other Hint Providers to align pod resources, such as CPU, SR-IOV VFs, and other device resources, for all Quality of Service (QoS) classes on the same non-uniform memory access (NUMA) node. This topology information and the configured Topology manager policy determine whether a workload is accepted or rejected on a node.

[NOTE]
====
To align CPU resources with other requested resources in a Pod spec, the CPU Manager must be enabled with the static CPU Manager policy.
====

The following Topology manager policies are available and dependent on the requirements of the workload can be enabled. For high performance workloads making use of SR-IOV VFs, NUMA awareness follows the NUMA node to which the SR-IOV capable network adapter is connected.

Best-effort policy::
For each container in a pod with the best-effort topology management policy, kubelet calls each Hint Provider to discover their resource availability. Using this information, the Topology Manager stores the preferred NUMA Node affinity for that container. If the affinity is not preferred, Topology Manager stores this and admits the pod to the node.

Restricted policy::
For each container in a pod with the restricted topology management policy, kubelet calls each Hint Provider to discover their resource availability. Using this information, the Topology Manager stores the preferred NUMA Node affinity for that container. If the affinity is not preferred, Topology Manager rejects this pod from the node, resulting in a pod in a Terminated state with a pod admission failure.

Single-numa-node policy::
For each container in a pod with the single-numa-node topology management policy, kubelet calls each Hint Provider to discover their resource availability. Using this information, the Topology Manager determines if a single NUMA Node affinity is possible. If it is, the pod is admitted to the node. If a single NUMA Node affinity is not possible, the Topology Manager rejects the pod from the node. This results in a pod in a Terminated state with a pod admission failure. For more information on Topology manager, see following link:#[OpenShift Documentation].

== IPv4 & IPv6

Applications should discover services via DNS by doing an AAAA and A query. If an application gets a AAAA response the application should prefer using the IPv6 address in the AAAA response for application sockets.

In OpenShift 4.7, you can declare ipFamilyPolicy: PreferDualStack which will present an IPv4 and IPv6 address in the service.

.CNF Recommendation
[IMPORTANT]
====
IPv4 *should* only be used inside a POD when absolutely necessary.
====

.CNF Recommendation
[IMPORTANT]
====
Services *should* be created as IPv6 only services wherever possible. If an application requires dual stack it should create a dual stack service.
====

link:https://kubernetes.io/docs/concepts/services-networking/dual-stack/[]

To configure IPv4/IPv6 dual-stack, set dual-stack cluster network assignments:

[source,yaml]
----
kube-apiserver:
  --service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>
----


== VRFs (aka routing instances)

VRFs provide a way to have separate routing tables on the device enabling multiple L3 routing domains concurrently. This allows for traffic in different VRF to be treated independently of each other.

== Ports reserved by OpenShift

The following ports are reserved by OpenShift and should NOT be used by any application. These ports are blocked by iptables on the nodes and traffic will not pass. Port list:

* `22623`
* `22624`

.CNF Requirement
[IMPORTANT]
====
The following ports are reserved by OpenShift and *must NOT be used* by any application: 22623, 22624
====
