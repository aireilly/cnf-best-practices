[id="cnf-best-practices-openshift-operations"]
= Operations that shall be executed by OpenShift

The application should not require NET_ADMIN capability to perform the following administrative operations:

== MTU setting

* For the cluster network, also known as the OVN or Openshift-SDN network, the MTU shall be configured by modifying the manifests generated by openshift-installer before deploying the cluster. Refer to link:https://docs.openshift.com/container-platform/4.7/networking/hardware_networks/using-SRIOV-multicast.html[] for more information.

* For the additional networks managed by the Cluster Network Operator, it can be configured through the NetworkAttachmentDefinition resources generated by the Cluster Network Operator. Refer to link:https://docs.openshift.com/container-platform/4.7/networking/multiple_networks/understanding-multiple-networks.html[] for more information.

* For the SR/IOV interfaces managed by the SRIOV Network Operator. Refer to link:https://docs.openshift.com/container-platform/4.7/networking/hardware_networks/configuring-SRIOV-device.html[] for more information.

== Link state modification

* All the links will be set up before attaching it to a pod.

== IP/MAC address assignment

* For all the networks, the IP/MAC address will be assigned to the interface during pod creation.

* MULTUS also allows users to override the IP/MAC address. Refer to link:https://docs.openshift.com/container-platform/4.7/networking/multiple_networks/attaching-pod.html#nw-MULTUS-advanced-annotations_attaching-pod[] for more information.

== Manipulate pod’s route table

* By default, the default route of the pod will point to the cluster network, with or without the additional networks. MULTUS also allows users to override the default route of the pod. Refer to link:https://docs.openshift.com/container-platform/4.7/networking/multiple_networks/attaching-pod.html#nw-MULTUS-advanced-annotations_attaching-pod[] for more information.

* Non-default routes can be added to pod routing tables by various IPAM CNI plugins during pod creation.

== SR/IOV VF setting

Besides the functions aforementioned, the SRIOV Network Operator supports configuring the following parameters for SRIOV VFs. Refer to link:https://docs.openshift.com/container-platform/4.7/networking/hardware_networks/configuring-SRIOV-net-attach.html[] for more information.

* vlan
* linkState
* maxTxRate
* minRxRate
* vlanQoS
* spoofChk
* trust

== Multicast

* In OCP, multicast is supported for both the default interface (OVN or OpenShift-SDN) and the additional interfaces (macvlan, SR-IOV...). However, multicast is disabled by default. To enable it please refer to link:https://docs.openshift.com/container-platform/4.7/networking/openshift_sdn/enabling-multicast.html[] and link:https://docs.openshift.com/container-platform/4.7/networking/hardware_networks/using-sriov-multicast.html#nw-using-an-sriov-interface-for-multicast_using-sriov-multicast[]
* If your application works as a multicast source and you want to utilize the additional interfaces to carry the multicast traffic, then you don’t need the NET_ADMIN capability. But you need to follow the instruction in link:https://docs.openshift.com/container-platform/4.3/networking/hardware_networks/using-sriov-multicast.html#nw-using-an-sriov-interface-for-multicast_using-sriov-multicast[] to set the correct multicast route in your pod’s routing table.

== Operations that can NOT be executed by OpenShift

All the CNI plugins will only be invoked during pod creation and deletion. If your CNF wants to perform any operations mentioned in the above chapter at runtime, the `NET_ADMIN` capability would be required. There are some other functionalities that are not currently supported by any of the OpenShift components which would also require `NET_ADMIN` capability:

* Link state modification at runtime

* IP/MAC modification at runtime

* Manipulate pod’s route table or firewall rules at runtime

* SR/IOV VF setting at runtime

* Netlink configuration

* For example, `ethtool` can be used to configure things like rxvlan, txvlan, gso, tso, etc.

* Multicast

** If your application works as a receiving member of IGMP groups, you need to specify the NET_ADMIN capability in the pod manifest. So that the app is allowed to assign multicast addresses to the pod interface and join an IGMP group.

* Set `SO_PRIORITY` to a socket to manipulate the 802.1p priority in ethernet frames

* Set `IP_TOS` to a socket to manipulate the DSCP value of IP packets

== Analyzing Your Application

To find out which capabilities the application needs, Red Hat developed a SystemTap script (container_check.stp). With this tool, the CNF developer can find out what capabilities an application requires in order to run in a container. It also shows the syscalls which were invoked. Find more info at link:https://linuxera.org/capabilities-seccomp-kubernetes/[]

Another tool is `capable` which is part of the BCC tools. It can be installed on RHEL8 with `dnf install bcc`.

=== Example

Here is an example of how to find out the capabilities that an application needs. `testpmd` is a DPDK based layer-2 forwarding application. It needs the CAP_IPC_LOCK to allocate the hugepage memory.

. Use container_check.stp. We can see CAP_IPC_LOCK and CAP_SYS_RAWIO are requested by `testpmd` and the relevant syscalls.
+
[source,terminal]
----
$ $ /usr/share/systemtap/examples/profiling/container_check.stp -c 'testpmd -l 1-2 -w 0000:00:09.0 -- -a --portmask=0x8 --nb-cores=1'
----
+
.Example output
[source,terminal]
----
[...]

capabilities used by executables

executable: prob capability

testpmd: cap_ipc_lock

testpmd: cap_sys_rawio

capabilities used by syscalls

executable, syscall ( capability ) : count testpmd, mlockall ( cap_ipc_lock ) : 1 testpmd, mmap ( cap_ipc_lock ) : 710 testpmd, open ( cap_sys_rawio ) : 1 testpmd, iopl ( cap_sys_rawio ) : 1

forbidden syscalls

executable, syscall: count

failed syscalls

executable, syscall = errno: count eal-intr-thread, epoll_wait = EINTR: 1 lcore-slave-2, read = 1 rte_mp_handle, recvmsg = 1 stapio, = EINTR: 1 stapio, execve = ENOENT: 3 stapio, rt_sigsuspend = 1 testpmd, flock = EAGAIN: 5 testpmd, stat = ENOENT: 10 testpmd, mkdir = EEXIST: 2 testpmd, readlink = ENOENT: 3 testpmd, access = ENOENT: 1141 testpmd, openat = ENOENT: 1 testpmd, open = ENOENT: 13 [...]
----

. Use capable command:
+
[source,terminal]
----
$ /usr/share/bcc/tools/capable
----

. Start the testpmd application from another terminal, and send some test traffic to it. $ testpmd -l 18-19 -w 0000:01:00.0 -- -a --portmask=0x1 --nb-cores=1

. Check the output of the ‘capable’ command. As we can see CAP_IPC_LOCK was requested for running `testpmd`.
+
[source,terminal]
----
[...]

00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 [...]
----

. Also, we can try to run `testpmd` without the CAP_IPC_LOCK with ‘capsh’. Now we can see that the hugepage memory cannot be allocated.

[source,terminal]
----
$ capsh --drop=cap_ipc_lock -- -c testpmd -l 18-19 -w 0000:01:00.0 -- -a --portmask=0x1 --nb-cores=1
----
+
.Example output
[source,terminal]
----
EAL: Detected 24 lcore(s)

EAL: Detected 2 NUMA nodes

EAL: Multi-process socket /var/run/dpdk/rte/mp_socket

EAL: No free hugepages reported in hugepages-1048576kB

EAL: Probing VFIO support...

EAL: VFIO support initialized

EAL: PCI device 0000:01:00.0 on NUMA socket 0

EAL: probe driver: 8086:10fb net_ixgbe

EAL: using IOMMU type 1 (Type 1)

EAL: Ignore mapping IO port bar(2)

EAL: PCI device 0000:01:00.1 on NUMA socket 0

EAL: probe driver: 8086:10fb net_ixgbe

EAL: PCI device 0000:07:00.0 on NUMA socket 0

EAL: probe driver: 8086:1521 net_e1000_igb

EAL: PCI device 0000:07:00.1 on NUMA socket 0

EAL: probe driver: 8086:1521 net_e1000_igb

EAL: cannot set up DMA remapping, error 12 (Cannot allocate memory) testpmd: mlockall() failed with error "Cannot allocate memory" testpmd: create a new mbuf pool <mbuf_pool_socket_0>: n=331456, size=2176, socket=0

testpmd: preferred mempool ops selected: ring_mp_mc

EAL: cannot set up DMA remapping, error 12 (Cannot allocate memory) testpmd: create a new mbuf pool <mbuf_pool_socket_1>: n=331456, size=2176,

socket=1

testpmd: preferred mempool ops selected: ring_mp_mc

EAL: cannot set up DMA remapping, error 12 (Cannot allocate memory) EAL: cannot set up DMA remapping, error 12 (Cannot allocate memory)
----

== CNF network security

CNFs must have the least permissions possible and CNFs must implement Network Policies that drop all traffic by default and permit only the relevant ports and protocols to the narrowest ranges of addresses possible.

.CNF Requirement
[IMPORTANT]
====
Applications must define network policies that permit only the minimum network access the application needs to function.
====

== Secrets management

Secrets objects in OpenShift provide a way to hold sensitive information such as passwords, config files and credentials. There are 4 types of secrets; service account, basic auth, ssh auth and TLS. Secrets can be added via deployment configurations or consumed by pods directly. For more information on secrets and examples, see the following documentation.

link:https://docs.openshift.com/container-platform/4.7/nodes/pods/nodes-pods-secrets.html[]

== SCC Permissions for an Application

Permissions to use an SCC is done by adding a cluster role that has _uses_ permissions for the SCC and then rolebindings for the users within a namespace to that role for users that need that SCC. Application admins can create their own role/rolebindings to assign permissions to a Service Account.

== User-Plane CNFs

A CNF which handles user plane traffic or latency-sensitive payloads at line rate falls into this category, such as load balancing, routing, deep packet inspection, and so on. Some of these CNFs may also need to process the packets at a lower level.

This kind of CNF may need to:

. Use SR-IOV interfaces.

. Fully or partially bypassing the kernel networking stack with userspace networking technologies, like DPDK, F-stack, VPP, OpenFastPath, etc. A userspace networking stack can not only improve the performance but also reduce the need for the ‘CAP_NET_ADMIN’ and ‘CAP_NET_RAW’.


[NOTE]
====
For Mellanox devices, those capabilities are requested if the application needs to configure the device(CAP_NET_ADMIN) and/or allocate raw ethernet queue through kernel drive(CAP_NET_RAW)
====

As ‘CAP_IPC_LOCK’ is mandatory for allocating hugepage memory, this capability shall be granted to the DPDK based applications. Additionally if the workload is latency-sensitive and needs the determinacy provided by the real-time kernel, the ‘CAP_SYS_NICE’ would also be required.

Here is an example pod manifest of a DPDK application:

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: dpdk-app
  namespace: <target_namespace>
  annotations:
    k8s.v1.cni.cncf.io/networks: dpdk-network
spec:
  containers:
  - name: testpmd
    image: <DPDK_image>
    securityContext:
      capabilities:
        add: ["IPC_LOCK"]
    volumeMounts:
    - mountPath: /dev/hugepages
      name: hugepage
    resources:
      limits:
      openshift.io/mlxnics: "1"
      memory: "1Gi"
      cpu: "4"
      hugepages-2Mi: "4Gi"
    requests:
      openshift.io/mlxnics: "1"
      memory: "1Gi"
      cpu: "4"
      hugepages-2Mi: "4Gi"
    command: ["sleep", "infinity"]
volumes:
- name: hugepage
  emptyDir:
    medium: HugePages
----

[source,yaml]
----
kind: SecurityContextConstraints
apiVersion: security.openshift.io/v1
metadata:
  name: cnfname
users: []
groups: []
priority: null
allowHostDirVolumePlugin: false
allowHostIPC: false
allowHostNetwork: false
allowHostPID: false
allowHostPorts: false
allowPrivilegeEscalation: true
allowPrivilegedContainer: false
allowedCapabilities: [IPC_LOCK, NET_ADMIN, NET_RAW] defaultAddCapabilities: null
requiredDropCapabilities:
- KILL
- MKNOD
- SETUID
- SETGID
fsGroup:
  type: MustRunAs
readOnlyRootFilesystem: false
runAsUser:
  type: MustRunAsRange
seLinuxContext:
  type: MustRunAs
supplementalGroups:
  type: RunAsAny
volumes:
- configMap
- downwardAPI
- emptyDir
- persistentVolumeClaim
- projected
- secret
----


