[id="cnf-best-practices-openshift-operations"]
= Operations that shall be executed by OpenShift

The application should not require `NET_ADMIN` capability to perform the following administrative operations:

[id="cnf-best-practices-mtu-setting"]
== Setting the MTU

* Configure the MTU for the cluster network, also known as the OVN or Openshift-SDN network, by modifying the manifests generated by `openshift-installer` before deploying the cluster. See link:https://docs.openshift.com/container-platform/latest/networking/changing-cluster-network-mtu.html[Changing the MTU for the cluster network] for more information.

* Configure additional networks managed by the Cluster Network Operator by using `NetworkAttachmentDefinition` resources generated by the Cluster Network Operator. See link:https://docs.openshift.com/container-platform/latest/networking/hardware_networks/using-sriov-multicast.html[Using high performance multicast] for more information.

* Configure SR-IOV interfaces by using the SR-IOV Network Operator, see link:https://docs.openshift.com/container-platform/latest/networking/hardware_networks/configuring-sriov-device.html[Configuring an SR-IOV network device] for more information.

[id="cnf-best-practices-link-state-modification"]
== Modifying link state

* All the links should be set up before attaching it to a pod.

[id="cnf-best-practices-ip/mac-address-assignment"]
== Assigning IP/MAC addresses

* For all the networks, the IP/MAC address should be assigned to the interface during pod creation.

* MULTUS also allows users to override the IP/MAC address. Refer to link:https://docs.openshift.com/container-platform/latest/networking/multiple_networks/attaching-pod.html[Attaching a pod to an additional network] for more information.

[id="cnf-best-practices-manipulate-pod’s-route-table"]
== Manipulating pod route tables

* By default, the default route of the pod will point to the cluster network, with or without the additional networks. MULTUS also allows users to override the default route of the pod. Refer to link:https://docs.openshift.com/container-platform/latest/networking/multiple_networks/attaching-pod.html[Attaching a pod to an additional network] for more information.

* Non-default routes can be added to pod routing tables by various IPAM CNI plugins during pod creation.

[id="cnf-best-practices-sr/iov-vf-setting"]
== Setting SR/IOV VFs

The SR-IOV Network Operator also supports configuring the following parameters for SR-IOV VFs. Refer to link:https://docs.openshift.com/container-platform/latest/networking/hardware_networks/configuring-sriov-net-attach.html[Configuring an SR-IOV Ethernet network attachment] for more information.

* `vlan`
* `linkState`
* `maxTxRate`
* `minRxRate`
* `vlanQoS`
* `spoofChk`
* `trust`

[id="cnf-best-practices-multicast"]
== Configuring Multicast

In OpenShift, multicast is supported for both the default interface (OVN or OpenShift-SDN) and the additional interfaces such as macvlan, SR-IOV, etc. Multicast is disabled by default. To enable it, refer to the following procedures:

* link:https://docs.openshift.com/container-platform/latest/networking/openshift_sdn/enabling-multicast.html[Enabling multicast for a project]
* link:https://docs.openshift.com/container-platform/latest/networking/hardware_networks/using-sriov-multicast.html#nw-using-an-sriov-interface-for-multicast_using-sriov-multicast[Configuring an SR-IOV interface for multicast]
* If your application works as a multicast source and you want to utilize the additional interfaces to carry the multicast traffic, then you don’t need the `NET_ADMIN` capability. Follow the instructions in link:https://docs.openshift.com/container-platform/latest/networking/hardware_networks/using-sriov-multicast.html[Using high performance multicast] to set the correct multicast route in the pod’s routing table.

[id="cnf-best-practices-operations-that-can-not-be-executed-by-openshift"]
== Operations that can not be executed by OpenShift

All the CNI plugins are only invoked during pod creation and deletion. If your CNF needs perform any operations mentioned above at runtime, the `NET_ADMIN` capability is required.

There are some other functionalities that are not currently supported by any of the OpenShift components which also require `NET_ADMIN` capability:

* Link state modification at runtime

* IP/MAC modification at runtime

* Manipulate pod’s route table or firewall rules at runtime

* SR/IOV VF setting at runtime

* Netlink configuration

* For example, `ethtool` can be used to configure things like rxvlan, txvlan, gso, tso, etc.

* Multicast
+
[NOTE]
====
If your application works as a receiving member of IGMP groups, you need to specify the NET_ADMIN capability in the pod manifest. So that the app is allowed to assign multicast addresses to the pod interface and join an IGMP group.
====

* Set `SO_PRIORITY` to a socket to manipulate the 802.1p priority in ethernet frames

* Set `IP_TOS` to a socket to manipulate the DSCP value of IP packets

[id="cnf-best-practices-analyzing-your-application"]
== Analyzing your application

To find out which capabilities the application needs, Red Hat has developed a SystemTap script (`container_check.stp`). With this tool, the CNF developer can find out what capabilities an application requires in order to run in a container. It also shows the syscalls which were invoked. Find more info at link:https://linuxera.org/capabilities-seccomp-kubernetes/[]

Another tool is `capable` which is part of the BCC tools. It can be installed on RHEL8 with `dnf install bcc`.

[id="cnf-best-practices-example"]
=== Finding the capabilities that an application needs

Here is an example of how to find out the capabilities that an application needs. `testpmd` is a DPDK based layer-2 forwarding application. It needs the `CAP_IPC_LOCK` to allocate the hugepage memory.

. Use container_check.stp. We can see `CAP_IPC_LOCK` and `CAP_SYS_RAWIO` are requested by `testpmd` and the relevant syscalls.
+
[source,terminal]
----
$ $ /usr/share/systemtap/examples/profiling/container_check.stp -c 'testpmd -l 1-2 -w 0000:00:09.0 -- -a --portmask=0x8 --nb-cores=1'
----
+
.Example output
[source,terminal]
----
[...]
capabilities used by executables
executable: prob capability
testpmd: cap_ipc_lock
testpmd: cap_sys_rawio
capabilities used by syscalls
executable, syscall ( capability ) : count testpmd, mlockall ( cap_ipc_lock ) : 1 testpmd, mmap ( cap_ipc_lock ) : 710 testpmd, open ( cap_sys_rawio ) : 1 testpmd, iopl ( cap_sys_rawio ) : 1
forbidden syscalls
executable, syscall: count
failed syscalls
executable, syscall [id="cnf-best-practices-errno:-count-eal-intr-thread,-epoll_wait-=-eintr:-1-lcore-slave-2,-read-=-1-rte_mp_handle,-recvmsg-=-1-stapio,-=-eintr:-1-stapio,-execve-=-enoent:-3-stapio,-rt_sigsuspend-=-1-testpmd,-flock-=-eagain:-5-testpmd,-stat-=-enoent:-10-testpmd,-mkdir-=-eexist:-2-testpmd,-readlink-=-enoent:-3-testpmd,-access-=-enoent:-1141-testpmd,-openat-=-enoent:-1-testpmd,-open-=-enoent:-13-[...]"]
= errno: count eal-intr-thread, epoll_wait = EINTR: 1 lcore-slave-2, read = 1 rte_mp_handle, recvmsg = 1 stapio, = EINTR: 1 stapio, execve = ENOENT: 3 stapio, rt_sigsuspend = 1 testpmd, flock = EAGAIN: 5 testpmd, stat = ENOENT: 10 testpmd, mkdir = EEXIST: 2 testpmd, readlink = ENOENT: 3 testpmd, access = ENOENT: 1141 testpmd, openat = ENOENT: 1 testpmd, open = ENOENT: 13 [...]
----

. Use the `capable` command:
+
[source,terminal]
----
$ /usr/share/bcc/tools/capable
----

. Start the testpmd application from another terminal, and send some test traffic to it. For example:
+
[source,terminal]
----
$ testpmd -l 18-19 -w 0000:01:00.0 -- -a --portmask=0x1 --nb-cores=1
----

. Check the output of the `capable` command. Below, `CAP_IPC_LOCK` was requested for running `testpmd`.
+
[source,terminal]
----
[...]

00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 00:41:58 0 3591 3591 testpmd 14 CAP_IPC_LOCK 1 [...]
----

. Also, try to run `testpmd` without `CAP_IPC_LOCK` set with `capsh`. Now we can see that the hugepage memory cannot be allocated.

[source,terminal]
----
$ capsh --drop=cap_ipc_lock -- -c testpmd -l 18-19 -w 0000:01:00.0 -- -a --portmask=0x1 --nb-cores=1
----
+
.Example output
[source,terminal]
----
EAL: Detected 24 lcore(s)
EAL: Detected 2 NUMA nodes
EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
EAL: No free hugepages reported in hugepages-1048576kB
EAL: Probing VFIO support...
EAL: VFIO support initialized
EAL: PCI device 0000:01:00.0 on NUMA socket 0
EAL: probe driver: 8086:10fb net_ixgbe
EAL: using IOMMU type 1 (Type 1)
EAL: Ignore mapping IO port bar(2)
EAL: PCI device 0000:01:00.1 on NUMA socket 0
EAL: probe driver: 8086:10fb net_ixgbe
EAL: PCI device 0000:07:00.0 on NUMA socket 0
EAL: probe driver: 8086:1521 net_e1000_igb
EAL: PCI device 0000:07:00.1 on NUMA socket 0
EAL: probe driver: 8086:1521 net_e1000_igb
EAL: cannot set up DMA remapping, error 12 (Cannot allocate memory) testpmd: mlockall() failed with error "Cannot allocate memory" testpmd: create a new mbuf pool <mbuf_pool_socket_0>: n=331456, size=2176, socket=0
testpmd: preferred mempool ops selected: ring_mp_mc
EAL: cannot set up DMA remapping, error 12 (Cannot allocate memory) testpmd: create a new mbuf pool <mbuf_pool_socket_1>: n=331456, size=2176,
socket=1
testpmd: preferred mempool ops selected: ring_mp_mc
EAL: cannot set up DMA remapping, error 12 (Cannot allocate memory) EAL: cannot set up DMA remapping, error 12 (Cannot allocate memory)
----

[id="cnf-best-practices-cnf-network-security"]
== Securing CNF networks

CNFs must have the least permissions possible and CNFs must implement Network Policies that drop all traffic by default and permit only the relevant ports and protocols to the narrowest ranges of addresses possible.

.CNF requirement
[IMPORTANT]
====
Applications must define network policies that permit only the minimum network access the application needs to function.
====

[id="cnf-best-practices-secrets-management"]
== Managing Secrets

Secrets objects in OpenShift provide a way to hold sensitive information such as passwords, config files and credentials. There are 4 types of secrets; service account, basic auth, ssh auth and TLS. Secrets can be added via deployment configurations or consumed by pods directly. For more information on secrets and examples, see the following documentation.

link:https://docs.openshift.com/container-platform/latest/nodes/pods/nodes-pods-secrets.html[Providing sensitive data to pods]

[id="cnf-best-practices-scc-permissions-for-an-application"]
== Setting SCC permissions for applications

Permissions to use an SCC is done by adding a cluster role that has _uses_ permissions for the SCC and then rolebindings for the users within a namespace to that role for users that need that SCC. Application admins can create their own role/rolebindings to assign permissions to a Service Account.

[id="cnf-best-practices-user-plane-cnfs"]
== Handling user-plane CNFs

A CNF which handles user plane traffic or latency-sensitive payloads at line rate falls into this category, such as load balancing, routing, deep packet inspection, and so on. Some of these CNFs may also need to process the packets at a lower level.

This kind of CNF may need to:

. Use SR-IOV interfaces

. Fully or partially bypassing the kernel networking stack with userspace networking technologies, like DPDK, F-stack, VPP, OpenFastPath, etc. A userspace networking stack can not only improve the performance but also reduce the need for the `CAP_NET_ADMIN` and `CAP_NET_RAW`.


[NOTE]
====
For Mellanox devices, those capabilities are requested if the application needs to configure the device(CAP_NET_ADMIN) and/or allocate raw ethernet queue through kernel drive(CAP_NET_RAW)
====

As `CAP_IPC_LOCK` is mandatory for allocating hugepage memory, this capability is granted to DPDK-based applications. Additionally, if the workload is latency-sensitive and needs the determinacy provided by the real-time kernel, the `CAP_SYS_NICE` is also required.

Here is an example pod manifest of a DPDK application:

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: dpdk-app
  namespace: <target_namespace>
  annotations:
    k8s.v1.cni.cncf.io/networks: dpdk-network
spec:
  containers:
  - name: testpmd
    image: <DPDK_image>
    securityContext:
      capabilities:
        add: ["IPC_LOCK"]
    volumeMounts:
    - mountPath: /dev/hugepages
      name: hugepage
    resources:
      limits:
      openshift.io/mlxnics: "1"
      memory: "1Gi"
      cpu: "4"
      hugepages-2Mi: "4Gi"
    requests:
      openshift.io/mlxnics: "1"
      memory: "1Gi"
      cpu: "4"
      hugepages-2Mi: "4Gi"
    command: ["sleep", "infinity"]
volumes:
- name: hugepage
  emptyDir:
    medium: HugePages
----

[source,yaml]
----
apiVersion: security.openshift.io/v1
kind: SecurityContextConstraints
metadata:
  name: cnfname
users: []
groups: []
priority: null
allowHostDirVolumePlugin: false
allowHostIPC: false
allowHostNetwork: false
allowHostPID: false
allowHostPorts: false
allowPrivilegeEscalation: true
allowPrivilegedContainer: false
allowedCapabilities: [IPC_LOCK, NET_ADMIN, NET_RAW] defaultAddCapabilities: null
requiredDropCapabilities:
- KILL
- MKNOD
- SETUID
- SETGID
fsGroup:
  type: MustRunAs
readOnlyRootFilesystem: false
runAsUser:
  type: MustRunAsRange
seLinuxContext:
  type: MustRunAs
supplementalGroups:
  type: RunAsAny
volumes:
- configMap
- downwardAPI
- emptyDir
- persistentVolumeClaim
- projected
- secret
----


